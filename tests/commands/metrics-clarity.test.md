# Metrics Clarity Command Test

## Test Input 1: User Engagement Analysis
```
/metrics-clarity
Data: DAU increased 15%, session duration decreased 8%, feature adoption at 23%
Context: New feature launched 4 weeks ago
Question: Is the feature successful?
Stakeholders: Engineering team wants to iterate, executives want ROI assessment
```

## Expected Output Validation:
- [ ] Identifies key question being answered
- [ ] Distinguishes correlation from causation
- [ ] Addresses potential data bias
- [ ] Provides clear conclusion upfront
- [ ] Uses plain language explanation
- [ ] Suggests actionable next steps

## Test Input 2: Conversion Funnel Analysis
```
/metrics-clarity
Data: Sign-up conversion 12%, activation 67%, retention 34%
Comparison: Industry benchmarks are 8%, 45%, 28%
Question: Where should we focus improvement efforts?
Timeline: Need recommendation for Q2 planning
```

## Expected Output Validation:
- [ ] Compares metrics in context
- [ ] Identifies highest-impact improvement area
- [ ] Explains reasoning clearly
- [ ] Quantifies potential improvement impact
- [ ] Addresses what could prove analysis wrong
- [ ] Provides specific recommendations

## Test Input 3: A/B Test Results
```
/metrics-clarity
Data: Variant A: 14.2% conversion, Variant B: 16.8% conversion
Sample size: 10,000 users each, 95% confidence
Question: Should we implement Variant B?
Concerns: Implementation complexity high, small absolute impact
```

## Expected Output Validation:
- [ ] Assesses statistical significance properly
- [ ] Calculates business impact clearly
- [ ] Weighs effort vs. benefit trade-off
- [ ] Addresses implementation considerations
- [ ] Provides clear recommendation
- [ ] Explains potential risks and mitigations