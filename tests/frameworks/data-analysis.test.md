# Data Analysis Framework Test

## Test Input 1: User Engagement Analysis
```
Context: We have user engagement data showing 40% monthly churn
Question: What's causing users to leave our platform?
Data: Page views, session duration, feature usage, support tickets
```

## Expected Output Validation:
- [ ] Leads with clear conclusion
- [ ] States question being answered (one sentence)
- [ ] Identifies what would prove analysis wrong
- [ ] Acknowledges potential bias
- [ ] Uses plain language (no jargon)
- [ ] Shows data visually when relevant
- [ ] Addresses potential objections to findings

## Test Input 2: Feature Adoption Metrics
```
Context: New feature launched 6 weeks ago, adoption at 12%
Question: Should we sunset this feature or double down?
Data: User segments, activation funnel, feedback scores
```

## Expected Output Validation:
- [ ] Clear recommendation upfront
- [ ] Explains what data supports/contradicts decision
- [ ] Identifies key stakeholders who need to believe analysis
- [ ] Explains potential misleading aspects of data
- [ ] Provides actionable next steps

## Test Input 3: Competitive Analysis
```
Context: Competitor launched similar feature, our conversion dropped 15%
Question: Is this correlation or causation?
Data: Market trends, user feedback, conversion funnel data
```

## Expected Output Validation:
- [ ] Distinguishes between correlation and causation
- [ ] One-sentence key finding
- [ ] Plain language explanation
- [ ] Shows supporting evidence clearly
- [ ] Addresses bias in analysis approach

## Edge Cases:
- **Limited data**: Framework should guide data collection strategy
- **Conflicting metrics**: Should help prioritize which metrics matter most
- **Stakeholder bias**: Should surface and address confirmation bias

## Success Criteria:
- Analysis can be explained in one clear sentence
- Recommendation directly connects to evidence
- Potential objections are preemptively addressed